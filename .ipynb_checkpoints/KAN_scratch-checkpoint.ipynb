{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d51549-4a10-4f9c-ab94-830a033eab9c",
   "metadata": {},
   "source": [
    "# KAN\n",
    "\n",
    "https://arxiv.org/html/2404.19756v1#abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "269ab6da-33f5-4538-b349-3d9872d42df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.interpolate as interp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86281872-41e3-40dd-bdb6-1a0ad4e3a03a",
   "metadata": {},
   "source": [
    "# Torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a0f59b0-d008-4f80-81fe-431efa419fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchSplineFunction(nn.Module):\n",
    "    def __init__(self, knots, coeffs):\n",
    "        super(TorchSplineFunction, self).__init__()\n",
    "        self.knots = nn.Parameter(torch.tensor(knots, dtype=torch.float32))\n",
    "        self.coeffs = nn.Parameter(torch.tensor(coeffs, dtype=torch.float32))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.clamp(x, min=self.knots.min(), max=self.knots.max())\n",
    "        distances = (x.unsqueeze(-1) - self.knots.unsqueeze(0)).abs()\n",
    "        weights = 1.0 / (distances + 1e-8)\n",
    "        weights = weights / weights.sum(dim=-1, keepdim=True)\n",
    "        return (weights * self.coeffs).sum(dim=-1)\n",
    "\n",
    "class TorchKANLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_knots):\n",
    "        super(TorchKANLayer, self).__init__()\n",
    "        self.splines = nn.ModuleList([TorchSplineFunction(self.initialize_knots(num_knots), \n",
    "                                                     self.initialize_coeffs(num_knots)) \n",
    "                                      for _ in range(input_dim * output_dim)])\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def initialize_knots(self, num_knots):\n",
    "        return np.linspace(0, 1, num_knots)\n",
    "\n",
    "    def initialize_coeffs(self, num_knots):\n",
    "        return np.random.randn(num_knots)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        outputs = []\n",
    "        for i in range(self.output_dim):\n",
    "            output = sum(self.splines[i * self.input_dim + j](x[:, j]) for j in range(self.input_dim))\n",
    "            outputs.append(output)\n",
    "        return torch.stack(outputs, dim=1).view(batch_size, self.output_dim)\n",
    "\n",
    "class TorchKAN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_knots):\n",
    "        super(TorchKAN, self).__init__()\n",
    "        layers = [TorchKANLayer(input_dim, hidden_dim, num_knots)]\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(TorchKANLayer(hidden_dim, hidden_dim, num_knots))\n",
    "        layers.append(TorchKANLayer(hidden_dim, output_dim, num_knots))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a3daf8ca-59c5-49aa-b09e-7abcf7c8bd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "        \n",
    "        accuracy = 100 * correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}, Accuracy: {accuracy}%\")\n",
    "\n",
    "def torch_evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader)}, Accuracy: {accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "06944195-eef0-4a9c-8f88-3ef4880313ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 2.86202871799469\n",
      "Epoch [2/50], Loss: 3.001336485147476\n",
      "Epoch [3/50], Loss: 3.301159769296646\n",
      "Epoch [4/50], Loss: 2.6966452300548553\n",
      "Epoch [5/50], Loss: 3.1048040091991425\n",
      "Epoch [6/50], Loss: 2.866089105606079\n",
      "Epoch [7/50], Loss: 2.9012086987495422\n",
      "Epoch [8/50], Loss: 2.6421312391757965\n",
      "Epoch [9/50], Loss: 2.3339592814445496\n",
      "Epoch [10/50], Loss: 2.4590850472450256\n",
      "Epoch [11/50], Loss: 2.377817839384079\n",
      "Epoch [12/50], Loss: 2.1627655178308487\n",
      "Epoch [13/50], Loss: 1.9259559214115143\n",
      "Epoch [14/50], Loss: 2.2630555480718613\n",
      "Epoch [15/50], Loss: 2.1690630316734314\n",
      "Epoch [16/50], Loss: 1.9556254893541336\n",
      "Epoch [17/50], Loss: 2.002310633659363\n",
      "Epoch [18/50], Loss: 1.8260233774781227\n",
      "Epoch [19/50], Loss: 1.8134599179029465\n",
      "Epoch [20/50], Loss: 1.6560426205396652\n",
      "Epoch [21/50], Loss: 2.0070666521787643\n",
      "Epoch [22/50], Loss: 1.7113595008850098\n",
      "Epoch [23/50], Loss: 1.9828859716653824\n",
      "Epoch [24/50], Loss: 1.9659209996461868\n",
      "Epoch [25/50], Loss: 2.195171818137169\n",
      "Epoch [26/50], Loss: 2.2549391239881516\n",
      "Epoch [27/50], Loss: 2.840280592441559\n",
      "Epoch [28/50], Loss: 2.4743815809488297\n",
      "Epoch [29/50], Loss: 2.3499794602394104\n",
      "Epoch [30/50], Loss: 2.1912893652915955\n",
      "Epoch [31/50], Loss: 1.9688952267169952\n",
      "Epoch [32/50], Loss: 2.347014859318733\n",
      "Epoch [33/50], Loss: 2.1650979220867157\n",
      "Epoch [34/50], Loss: 2.3321551978588104\n",
      "Epoch [35/50], Loss: 2.3981993794441223\n",
      "Epoch [36/50], Loss: 1.9823937863111496\n",
      "Epoch [37/50], Loss: 2.1509096324443817\n",
      "Epoch [38/50], Loss: 1.685272440314293\n",
      "Epoch [39/50], Loss: 1.7285784482955933\n",
      "Epoch [40/50], Loss: 1.6475194618105888\n",
      "Epoch [41/50], Loss: 1.5585505664348602\n",
      "Epoch [42/50], Loss: 2.0945367962121964\n",
      "Epoch [43/50], Loss: 1.6966592073440552\n",
      "Epoch [44/50], Loss: 1.619852028787136\n",
      "Epoch [45/50], Loss: 1.3737515285611153\n",
      "Epoch [46/50], Loss: 1.4495082795619965\n",
      "Epoch [47/50], Loss: 1.3346683904528618\n",
      "Epoch [48/50], Loss: 1.4700817167758942\n",
      "Epoch [49/50], Loss: 1.5960895717144012\n",
      "Epoch [50/50], Loss: 1.3381153717637062\n",
      "Test Loss: 1.2771149277687073\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 20\n",
    "output_dim = 3\n",
    "num_layers = 5\n",
    "num_knots = 20\n",
    "num_epochs = 50\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # Note: No unsqueeze for CrossEntropyLoss\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)  # Note: No unsqueeze for CrossEntropyLoss\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "model = TorchKAN(input_dim, hidden_dim, output_dim, num_layers, num_knots)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "torch_train_model(model, train_loader, criterion, optimizer, num_epochs)\n",
    "torch_evaluate_model(model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94429d-48fe-422d-a887-7f7566d48634",
   "metadata": {},
   "source": [
    "# TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1ebecf88-96bf-42df-87cc-770fd4201eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TensorFlowSplineFunction(Layer):\n",
    "    def __init__(self, knots, coeffs):\n",
    "        super(TensorFlowSplineFunction, self).__init__()\n",
    "        self.knots = tf.Variable(knots, trainable=True, dtype=tf.float32)\n",
    "        self.coeffs = tf.Variable(coeffs, trainable=True, dtype=tf.float32)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = tf.clip_by_value(x, tf.reduce_min(self.knots), tf.reduce_max(self.knots))\n",
    "        distances = tf.abs(x[:, tf.newaxis] - self.knots[tf.newaxis, :])\n",
    "        weights = 1.0 / (distances + 1e-8)\n",
    "        weights = weights / tf.reduce_sum(weights, axis=-1, keepdims=True)\n",
    "        return tf.reduce_sum(weights * self.coeffs, axis=-1)\n",
    "\n",
    "class TensorFlowKANLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim, num_knots):\n",
    "        super(TensorFlowKANLayer, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.splines = [\n",
    "            TensorFlowSplineFunction(\n",
    "                tf.linspace(0.0, 1.0, num_knots),\n",
    "                tf.random.normal((num_knots,))\n",
    "            )\n",
    "            for _ in range(input_dim * output_dim)\n",
    "        ]\n",
    "\n",
    "    def call(self, x):\n",
    "        outputs = []\n",
    "        for i in range(self.output_dim):\n",
    "            output = sum(self.splines[i * self.input_dim + j](x[:, j])\n",
    "                         for j in range(self.input_dim))\n",
    "            outputs.append(output)\n",
    "        return tf.stack(outputs, axis=1)\n",
    "\n",
    "class TensorFlowKAN(Model):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers, num_knots):\n",
    "        super(TensorFlowKAN, self).__init__()\n",
    "        self.layers_list = [TensorFlowKANLayer(input_dim, hidden_dim, num_knots)]\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers_list.append(TensorFlowKANLayer(hidden_dim, hidden_dim, num_knots))\n",
    "        self.layers_list.append(TensorFlowKANLayer(hidden_dim, output_dim, num_knots))\n",
    "\n",
    "    def call(self, x):\n",
    "        for layer in self.layers_list:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5758b515-4707-4afd-8fa1-4b38ee6a55a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "8/8 [==============================] - 143s 41ms/step - loss: 2.2178 - accuracy: 0.3333\n",
      "Epoch 2/500\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.0350 - accuracy: 0.5750\n",
      "Epoch 3/500\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.9252 - accuracy: 0.6667\n",
      "Epoch 4/500\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 1.2831 - accuracy: 0.5083\n",
      "Epoch 5/500\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 1.3170 - accuracy: 0.5583\n",
      "Epoch 6/500\n",
      "8/8 [==============================] - 0s 41ms/step - loss: 0.7117 - accuracy: 0.7167\n",
      "Epoch 7/500\n",
      "8/8 [==============================] - 0s 42ms/step - loss: 0.5222 - accuracy: 0.7833\n",
      "Epoch 8/500\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.6043 - accuracy: 0.7583\n",
      "Epoch 9/500\n",
      "8/8 [==============================] - 0s 40ms/step - loss: 0.6001 - accuracy: 0.6917\n",
      "Epoch 10/500\n",
      "8/8 [==============================] - 0s 48ms/step - loss: 0.5528 - accuracy: 0.7917\n",
      "Epoch 11/500\n",
      "8/8 [==============================] - 0s 39ms/step - loss: 0.5062 - accuracy: 0.8083\n",
      "Epoch 12/500\n",
      "8/8 [==============================] - 0s 43ms/step - loss: 0.6461 - accuracy: 0.7417\n",
      "Epoch 13/500\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.6517 - accuracy: 0.7000\n",
      "Epoch 14/500\n",
      "8/8 [==============================] - 0s 52ms/step - loss: 0.5058 - accuracy: 0.8083\n",
      "Epoch 15/500\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.3482 - accuracy: 0.8500\n",
      "Epoch 16/500\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.2965 - accuracy: 0.8250\n",
      "Epoch 17/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3145 - accuracy: 0.9000\n",
      "Epoch 18/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.4940 - accuracy: 0.8583\n",
      "Epoch 19/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.4202 - accuracy: 0.8500\n",
      "Epoch 20/500\n",
      "8/8 [==============================] - 0s 54ms/step - loss: 0.5863 - accuracy: 0.7667\n",
      "Epoch 21/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.4823 - accuracy: 0.8083\n",
      "Epoch 22/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.5010 - accuracy: 0.7917\n",
      "Epoch 23/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5024 - accuracy: 0.7833\n",
      "Epoch 24/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5369 - accuracy: 0.7417\n",
      "Epoch 25/500\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.5021 - accuracy: 0.7583\n",
      "Epoch 26/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.4626 - accuracy: 0.7667\n",
      "Epoch 27/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5193 - accuracy: 0.7917\n",
      "Epoch 28/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.4791 - accuracy: 0.8750\n",
      "Epoch 29/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.5924 - accuracy: 0.8167\n",
      "Epoch 30/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.6155 - accuracy: 0.7667\n",
      "Epoch 31/500\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.4982 - accuracy: 0.8333\n",
      "Epoch 32/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3128 - accuracy: 0.8583\n",
      "Epoch 33/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.3043 - accuracy: 0.8500\n",
      "Epoch 34/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2808 - accuracy: 0.8833\n",
      "Epoch 35/500\n",
      "8/8 [==============================] - 0s 55ms/step - loss: 0.2163 - accuracy: 0.9083\n",
      "Epoch 36/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1868 - accuracy: 0.9250\n",
      "Epoch 37/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2155 - accuracy: 0.9083\n",
      "Epoch 38/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2459 - accuracy: 0.8917\n",
      "Epoch 39/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2331 - accuracy: 0.8667\n",
      "Epoch 40/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.2074 - accuracy: 0.9250\n",
      "Epoch 41/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.2070 - accuracy: 0.9167\n",
      "Epoch 42/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.3385 - accuracy: 0.8417\n",
      "Epoch 43/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2647 - accuracy: 0.8833\n",
      "Epoch 44/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2385 - accuracy: 0.8833\n",
      "Epoch 45/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1609 - accuracy: 0.9583\n",
      "Epoch 46/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2179 - accuracy: 0.9417\n",
      "Epoch 47/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2500 - accuracy: 0.9083\n",
      "Epoch 48/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2006 - accuracy: 0.9417\n",
      "Epoch 49/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1532 - accuracy: 0.9583\n",
      "Epoch 50/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1139 - accuracy: 0.9667\n",
      "Epoch 51/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1296 - accuracy: 0.9667\n",
      "Epoch 52/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1159 - accuracy: 0.9583\n",
      "Epoch 53/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0975 - accuracy: 0.9667\n",
      "Epoch 54/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1088 - accuracy: 0.9583\n",
      "Epoch 55/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1071 - accuracy: 0.9583\n",
      "Epoch 56/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0737 - accuracy: 0.9833\n",
      "Epoch 57/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0671 - accuracy: 0.9750\n",
      "Epoch 58/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.1064 - accuracy: 0.9417\n",
      "Epoch 59/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0734 - accuracy: 0.9667\n",
      "Epoch 60/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0615 - accuracy: 0.9833\n",
      "Epoch 61/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0530 - accuracy: 0.9833\n",
      "Epoch 62/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0678 - accuracy: 0.9833\n",
      "Epoch 63/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0600 - accuracy: 0.9833\n",
      "Epoch 64/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0634 - accuracy: 0.9833\n",
      "Epoch 65/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0485 - accuracy: 0.9833\n",
      "Epoch 66/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0397 - accuracy: 0.9833\n",
      "Epoch 67/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0382 - accuracy: 0.9833\n",
      "Epoch 68/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0355 - accuracy: 0.9917\n",
      "Epoch 69/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0396 - accuracy: 0.9917\n",
      "Epoch 70/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0268 - accuracy: 0.9917\n",
      "Epoch 71/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0226 - accuracy: 0.9917\n",
      "Epoch 72/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0294 - accuracy: 0.9917\n",
      "Epoch 73/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0366 - accuracy: 0.9917\n",
      "Epoch 74/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0235 - accuracy: 1.0000\n",
      "Epoch 75/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0329 - accuracy: 0.9917\n",
      "Epoch 76/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0227 - accuracy: 1.0000\n",
      "Epoch 77/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0526 - accuracy: 0.9750\n",
      "Epoch 78/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0363 - accuracy: 0.9917\n",
      "Epoch 79/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0337 - accuracy: 0.9917\n",
      "Epoch 80/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0153 - accuracy: 1.0000\n",
      "Epoch 81/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0108 - accuracy: 1.0000\n",
      "Epoch 82/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0092 - accuracy: 1.0000\n",
      "Epoch 83/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 84/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0272 - accuracy: 0.9917\n",
      "Epoch 85/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0169 - accuracy: 0.9917\n",
      "Epoch 86/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0158 - accuracy: 0.9917\n",
      "Epoch 87/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0158 - accuracy: 0.9917\n",
      "Epoch 88/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0430 - accuracy: 0.9833\n",
      "Epoch 89/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0749 - accuracy: 0.9750\n",
      "Epoch 90/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0554 - accuracy: 0.9750\n",
      "Epoch 91/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0189 - accuracy: 0.9917\n",
      "Epoch 92/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0351 - accuracy: 0.9833\n",
      "Epoch 93/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0603 - accuracy: 0.9750\n",
      "Epoch 94/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0384 - accuracy: 0.9833\n",
      "Epoch 95/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.1136 - accuracy: 0.9667\n",
      "Epoch 96/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1133 - accuracy: 0.9500\n",
      "Epoch 97/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0906 - accuracy: 0.9500\n",
      "Epoch 98/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0831 - accuracy: 0.9750\n",
      "Epoch 99/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1881 - accuracy: 0.9083\n",
      "Epoch 100/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.3116 - accuracy: 0.8833\n",
      "Epoch 101/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2221 - accuracy: 0.9083\n",
      "Epoch 102/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.2905 - accuracy: 0.8750\n",
      "Epoch 103/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2031 - accuracy: 0.9500\n",
      "Epoch 104/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2056 - accuracy: 0.9000\n",
      "Epoch 105/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2178 - accuracy: 0.8750\n",
      "Epoch 106/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2740 - accuracy: 0.8750\n",
      "Epoch 107/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2328 - accuracy: 0.8667\n",
      "Epoch 108/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2486 - accuracy: 0.8833\n",
      "Epoch 109/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2334 - accuracy: 0.8917\n",
      "Epoch 110/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.1894 - accuracy: 0.9417\n",
      "Epoch 111/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1943 - accuracy: 0.9167\n",
      "Epoch 112/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1720 - accuracy: 0.9500\n",
      "Epoch 113/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1438 - accuracy: 0.9333\n",
      "Epoch 114/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1237 - accuracy: 0.9500\n",
      "Epoch 115/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1064 - accuracy: 0.9667\n",
      "Epoch 116/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.2550 - accuracy: 0.9167\n",
      "Epoch 117/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.1551 - accuracy: 0.9500\n",
      "Epoch 118/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1420 - accuracy: 0.9333\n",
      "Epoch 119/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0957 - accuracy: 0.9500\n",
      "Epoch 120/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1818 - accuracy: 0.9250\n",
      "Epoch 121/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1508 - accuracy: 0.9500\n",
      "Epoch 122/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0958 - accuracy: 0.9583\n",
      "Epoch 123/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0760 - accuracy: 0.9750\n",
      "Epoch 124/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0795 - accuracy: 0.9750\n",
      "Epoch 125/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0914 - accuracy: 0.9583\n",
      "Epoch 126/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0675 - accuracy: 0.9750\n",
      "Epoch 127/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1169 - accuracy: 0.9500\n",
      "Epoch 128/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.1124 - accuracy: 0.9667\n",
      "Epoch 129/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0940 - accuracy: 0.9500\n",
      "Epoch 130/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0807 - accuracy: 0.9833\n",
      "Epoch 131/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0749 - accuracy: 0.9750\n",
      "Epoch 132/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0479 - accuracy: 0.9833\n",
      "Epoch 133/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0299 - accuracy: 0.9917\n",
      "Epoch 134/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0203 - accuracy: 0.9917\n",
      "Epoch 135/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0179 - accuracy: 1.0000\n",
      "Epoch 136/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0109 - accuracy: 1.0000\n",
      "Epoch 137/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0104 - accuracy: 1.0000\n",
      "Epoch 138/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 139/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0073 - accuracy: 1.0000\n",
      "Epoch 140/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0081 - accuracy: 1.0000\n",
      "Epoch 141/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 142/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0080 - accuracy: 1.0000\n",
      "Epoch 143/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 144/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0074 - accuracy: 1.0000\n",
      "Epoch 145/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 146/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0239 - accuracy: 0.9917\n",
      "Epoch 147/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0309 - accuracy: 0.9917\n",
      "Epoch 148/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0243 - accuracy: 0.9917\n",
      "Epoch 149/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0491 - accuracy: 0.9833\n",
      "Epoch 150/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0399 - accuracy: 0.9917\n",
      "Epoch 151/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0817 - accuracy: 0.9833\n",
      "Epoch 152/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0111 - accuracy: 1.0000\n",
      "Epoch 153/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 154/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0053 - accuracy: 1.0000\n",
      "Epoch 155/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0290 - accuracy: 0.9917\n",
      "Epoch 156/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0061 - accuracy: 1.0000\n",
      "Epoch 157/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0092 - accuracy: 0.9917\n",
      "Epoch 158/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0620 - accuracy: 0.9833\n",
      "Epoch 159/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0186 - accuracy: 1.0000\n",
      "Epoch 160/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0352 - accuracy: 0.9917\n",
      "Epoch 161/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 162/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 163/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.2037 - accuracy: 0.9333\n",
      "Epoch 164/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0851 - accuracy: 0.9667\n",
      "Epoch 165/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.1838 - accuracy: 0.9667\n",
      "Epoch 166/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0206 - accuracy: 0.9917\n",
      "Epoch 167/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0652 - accuracy: 0.9500\n",
      "Epoch 168/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.1433 - accuracy: 0.9417\n",
      "Epoch 169/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2354 - accuracy: 0.9417\n",
      "Epoch 170/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.3237 - accuracy: 0.9333\n",
      "Epoch 171/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1499 - accuracy: 0.9667\n",
      "Epoch 172/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2091 - accuracy: 0.9417\n",
      "Epoch 173/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1594 - accuracy: 0.9417\n",
      "Epoch 174/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.2814 - accuracy: 0.9250\n",
      "Epoch 175/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1826 - accuracy: 0.9583\n",
      "Epoch 176/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1365 - accuracy: 0.9750\n",
      "Epoch 177/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1050 - accuracy: 0.9750\n",
      "Epoch 178/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0764 - accuracy: 0.9750\n",
      "Epoch 179/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0762 - accuracy: 0.9750\n",
      "Epoch 180/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1054 - accuracy: 0.9667\n",
      "Epoch 181/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0847 - accuracy: 0.9750\n",
      "Epoch 182/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0884 - accuracy: 0.9750\n",
      "Epoch 183/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0956 - accuracy: 0.9750\n",
      "Epoch 184/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1123 - accuracy: 0.9417\n",
      "Epoch 185/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1980 - accuracy: 0.9417\n",
      "Epoch 186/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.2179 - accuracy: 0.9250\n",
      "Epoch 187/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.1658 - accuracy: 0.9500\n",
      "Epoch 188/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.2126 - accuracy: 0.9583\n",
      "Epoch 189/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.1015 - accuracy: 0.9833\n",
      "Epoch 190/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.1125 - accuracy: 0.9583\n",
      "Epoch 191/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0789 - accuracy: 0.9833\n",
      "Epoch 192/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0992 - accuracy: 0.9750\n",
      "Epoch 193/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0669 - accuracy: 0.9667\n",
      "Epoch 194/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0484 - accuracy: 0.9833\n",
      "Epoch 195/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0747 - accuracy: 0.9833\n",
      "Epoch 196/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0488 - accuracy: 0.9833\n",
      "Epoch 197/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0411 - accuracy: 0.9833\n",
      "Epoch 198/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0353 - accuracy: 0.9917\n",
      "Epoch 199/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0341 - accuracy: 0.9917\n",
      "Epoch 200/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0343 - accuracy: 0.9917\n",
      "Epoch 201/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0286 - accuracy: 0.9917\n",
      "Epoch 202/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0293 - accuracy: 0.9917\n",
      "Epoch 203/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0300 - accuracy: 0.9917\n",
      "Epoch 204/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0539 - accuracy: 0.9833\n",
      "Epoch 205/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0606 - accuracy: 0.9833\n",
      "Epoch 206/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0299 - accuracy: 0.9917\n",
      "Epoch 207/500\n",
      "8/8 [==============================] - 0s 56ms/step - loss: 0.0430 - accuracy: 0.9917\n",
      "Epoch 208/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0377 - accuracy: 0.9917\n",
      "Epoch 209/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0366 - accuracy: 0.9917\n",
      "Epoch 210/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0174 - accuracy: 1.0000\n",
      "Epoch 211/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0196 - accuracy: 0.9917\n",
      "Epoch 212/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0163 - accuracy: 1.0000\n",
      "Epoch 213/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0133 - accuracy: 1.0000\n",
      "Epoch 214/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0204 - accuracy: 0.9917\n",
      "Epoch 215/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0350 - accuracy: 0.9917\n",
      "Epoch 216/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0241 - accuracy: 0.9917\n",
      "Epoch 217/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0394 - accuracy: 0.9833\n",
      "Epoch 218/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0332 - accuracy: 0.9833\n",
      "Epoch 219/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0374 - accuracy: 0.9833\n",
      "Epoch 220/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0237 - accuracy: 0.9833\n",
      "Epoch 221/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0225 - accuracy: 0.9917\n",
      "Epoch 222/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0198 - accuracy: 0.9833\n",
      "Epoch 223/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0189 - accuracy: 0.9917\n",
      "Epoch 224/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0182 - accuracy: 0.9917\n",
      "Epoch 225/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0329 - accuracy: 0.9750\n",
      "Epoch 226/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0328 - accuracy: 0.9833\n",
      "Epoch 227/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0114 - accuracy: 1.0000\n",
      "Epoch 228/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0096 - accuracy: 1.0000\n",
      "Epoch 229/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0203 - accuracy: 0.9833\n",
      "Epoch 230/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0099 - accuracy: 1.0000\n",
      "Epoch 231/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0253 - accuracy: 0.9917\n",
      "Epoch 232/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0893 - accuracy: 0.9750\n",
      "Epoch 233/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.1363 - accuracy: 0.9667\n",
      "Epoch 234/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0312 - accuracy: 0.9833\n",
      "Epoch 235/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0398 - accuracy: 0.9750\n",
      "Epoch 236/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.1004 - accuracy: 0.9667\n",
      "Epoch 237/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 0.0397 - accuracy: 0.9833\n",
      "Epoch 238/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0356 - accuracy: 0.9833\n",
      "Epoch 239/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0263 - accuracy: 0.9917\n",
      "Epoch 240/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0624 - accuracy: 0.9833\n",
      "Epoch 241/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0177 - accuracy: 0.9917\n",
      "Epoch 242/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0128 - accuracy: 1.0000\n",
      "Epoch 243/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0130 - accuracy: 1.0000\n",
      "Epoch 244/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0379 - accuracy: 0.9833\n",
      "Epoch 245/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.1158 - accuracy: 0.9667\n",
      "Epoch 246/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0798 - accuracy: 0.9583\n",
      "Epoch 247/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.2085 - accuracy: 0.9667\n",
      "Epoch 248/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0427 - accuracy: 0.9917\n",
      "Epoch 249/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0882 - accuracy: 0.9667\n",
      "Epoch 250/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0587 - accuracy: 0.9750\n",
      "Epoch 251/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0310 - accuracy: 0.9917\n",
      "Epoch 252/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0423 - accuracy: 0.9750\n",
      "Epoch 253/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0292 - accuracy: 0.9917\n",
      "Epoch 254/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0244 - accuracy: 0.9833\n",
      "Epoch 255/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0249 - accuracy: 0.9917\n",
      "Epoch 256/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0199 - accuracy: 0.9833\n",
      "Epoch 257/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0151 - accuracy: 1.0000\n",
      "Epoch 258/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0129 - accuracy: 1.0000\n",
      "Epoch 259/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0122 - accuracy: 1.0000\n",
      "Epoch 260/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0091 - accuracy: 1.0000\n",
      "Epoch 261/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0084 - accuracy: 1.0000\n",
      "Epoch 262/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 263/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0079 - accuracy: 1.0000\n",
      "Epoch 264/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 265/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 0.0082 - accuracy: 1.0000\n",
      "Epoch 266/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 267/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0077 - accuracy: 1.0000\n",
      "Epoch 268/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 269/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0069 - accuracy: 1.0000\n",
      "Epoch 270/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 271/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0062 - accuracy: 1.0000\n",
      "Epoch 272/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.0063 - accuracy: 1.0000\n",
      "Epoch 273/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0057 - accuracy: 1.0000\n",
      "Epoch 274/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 275/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0054 - accuracy: 1.0000\n",
      "Epoch 276/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 277/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 0.0049 - accuracy: 1.0000\n",
      "Epoch 278/500\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 0.0050 - accuracy: 1.0000\n",
      "Epoch 279/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 280/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0043 - accuracy: 1.0000\n",
      "Epoch 281/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 0.0044 - accuracy: 1.0000\n",
      "Epoch 282/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 283/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 284/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 285/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 286/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 287/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 288/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 289/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 290/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 291/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 292/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0027 - accuracy: 1.0000\n",
      "Epoch 293/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 294/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 295/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 296/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 297/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0025 - accuracy: 1.0000\n",
      "Epoch 298/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 299/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0023 - accuracy: 1.0000\n",
      "Epoch 300/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 301/500\n",
      "8/8 [==============================] - 1s 61ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 302/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 303/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 304/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 305/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 306/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 307/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 308/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 309/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 310/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 311/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 312/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 313/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 314/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 315/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 316/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 317/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 318/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 319/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 320/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 321/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 322/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 323/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 324/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 325/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 326/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 327/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 328/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 9.9483e-04 - accuracy: 1.0000\n",
      "Epoch 329/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 330/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 331/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 9.7751e-04 - accuracy: 1.0000\n",
      "Epoch 332/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 333/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 9.5834e-04 - accuracy: 1.0000\n",
      "Epoch 334/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.9000e-04 - accuracy: 1.0000\n",
      "Epoch 335/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 9.2810e-04 - accuracy: 1.0000\n",
      "Epoch 336/500\n",
      "8/8 [==============================] - 10s 61ms/step - loss: 9.1463e-04 - accuracy: 1.0000\n",
      "Epoch 337/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 9.3019e-04 - accuracy: 1.0000\n",
      "Epoch 338/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.8316e-04 - accuracy: 1.0000\n",
      "Epoch 339/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 9.5458e-04 - accuracy: 1.0000\n",
      "Epoch 340/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.3091e-04 - accuracy: 1.0000\n",
      "Epoch 341/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 9.7951e-04 - accuracy: 1.0000\n",
      "Epoch 342/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 8.0584e-04 - accuracy: 1.0000\n",
      "Epoch 343/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 8.8293e-04 - accuracy: 1.0000\n",
      "Epoch 344/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 8.2194e-04 - accuracy: 1.0000\n",
      "Epoch 345/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 7.9624e-04 - accuracy: 1.0000\n",
      "Epoch 346/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 7.9660e-04 - accuracy: 1.0000\n",
      "Epoch 347/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 7.9039e-04 - accuracy: 1.0000\n",
      "Epoch 348/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.7318e-04 - accuracy: 1.0000\n",
      "Epoch 349/500\n",
      "8/8 [==============================] - 1s 76ms/step - loss: 7.9008e-04 - accuracy: 1.0000\n",
      "Epoch 350/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 8.1221e-04 - accuracy: 1.0000\n",
      "Epoch 351/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 7.5039e-04 - accuracy: 1.0000\n",
      "Epoch 352/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 9.8608e-04 - accuracy: 1.0000\n",
      "Epoch 353/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 8.6191e-04 - accuracy: 1.0000\n",
      "Epoch 354/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 7.6562e-04 - accuracy: 1.0000\n",
      "Epoch 355/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.7847e-04 - accuracy: 1.0000\n",
      "Epoch 356/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 7.0256e-04 - accuracy: 1.0000\n",
      "Epoch 357/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 7.3327e-04 - accuracy: 1.0000\n",
      "Epoch 358/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 7.3594e-04 - accuracy: 1.0000\n",
      "Epoch 359/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 7.3358e-04 - accuracy: 1.0000\n",
      "Epoch 360/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.4113e-04 - accuracy: 1.0000\n",
      "Epoch 361/500\n",
      "8/8 [==============================] - 0s 57ms/step - loss: 6.8174e-04 - accuracy: 1.0000\n",
      "Epoch 362/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.5133e-04 - accuracy: 1.0000\n",
      "Epoch 363/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 6.8632e-04 - accuracy: 1.0000\n",
      "Epoch 364/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 8.0799e-04 - accuracy: 1.0000\n",
      "Epoch 365/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 7.5871e-04 - accuracy: 1.0000\n",
      "Epoch 366/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.0152e-04 - accuracy: 1.0000\n",
      "Epoch 367/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 6.6517e-04 - accuracy: 1.0000\n",
      "Epoch 368/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 6.6411e-04 - accuracy: 1.0000\n",
      "Epoch 369/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 6.6276e-04 - accuracy: 1.0000\n",
      "Epoch 370/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 6.3432e-04 - accuracy: 1.0000\n",
      "Epoch 371/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 6.1713e-04 - accuracy: 1.0000\n",
      "Epoch 372/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 6.1876e-04 - accuracy: 1.0000\n",
      "Epoch 373/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.3847e-04 - accuracy: 1.0000\n",
      "Epoch 374/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 6.0762e-04 - accuracy: 1.0000\n",
      "Epoch 375/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 7.0493e-04 - accuracy: 1.0000\n",
      "Epoch 376/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 5.8509e-04 - accuracy: 1.0000\n",
      "Epoch 377/500\n",
      "8/8 [==============================] - 0s 63ms/step - loss: 5.8179e-04 - accuracy: 1.0000\n",
      "Epoch 378/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 5.8056e-04 - accuracy: 1.0000\n",
      "Epoch 379/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 6.1333e-04 - accuracy: 1.0000\n",
      "Epoch 380/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 5.6289e-04 - accuracy: 1.0000\n",
      "Epoch 381/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 5.9105e-04 - accuracy: 1.0000\n",
      "Epoch 382/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 5.4956e-04 - accuracy: 1.0000\n",
      "Epoch 383/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 5.9492e-04 - accuracy: 1.0000\n",
      "Epoch 384/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 6.0691e-04 - accuracy: 1.0000\n",
      "Epoch 385/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 6.0722e-04 - accuracy: 1.0000\n",
      "Epoch 386/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 5.2842e-04 - accuracy: 1.0000\n",
      "Epoch 387/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 5.1645e-04 - accuracy: 1.0000\n",
      "Epoch 388/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 5.4490e-04 - accuracy: 1.0000\n",
      "Epoch 389/500\n",
      "8/8 [==============================] - 1s 72ms/step - loss: 4.9834e-04 - accuracy: 1.0000\n",
      "Epoch 390/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 5.1436e-04 - accuracy: 1.0000\n",
      "Epoch 391/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 5.3357e-04 - accuracy: 1.0000\n",
      "Epoch 392/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 5.0642e-04 - accuracy: 1.0000\n",
      "Epoch 393/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 5.4454e-04 - accuracy: 1.0000\n",
      "Epoch 394/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.9999e-04 - accuracy: 1.0000\n",
      "Epoch 395/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 4.6706e-04 - accuracy: 1.0000\n",
      "Epoch 396/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 4.6478e-04 - accuracy: 1.0000\n",
      "Epoch 397/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 4.9576e-04 - accuracy: 1.0000\n",
      "Epoch 398/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 4.5559e-04 - accuracy: 1.0000\n",
      "Epoch 399/500\n",
      "8/8 [==============================] - 1s 67ms/step - loss: 5.2016e-04 - accuracy: 1.0000\n",
      "Epoch 400/500\n",
      "8/8 [==============================] - 1s 70ms/step - loss: 4.5325e-04 - accuracy: 1.0000\n",
      "Epoch 401/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 5.1290e-04 - accuracy: 1.0000\n",
      "Epoch 402/500\n",
      "8/8 [==============================] - 1s 69ms/step - loss: 4.6000e-04 - accuracy: 1.0000\n",
      "Epoch 403/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.6205e-04 - accuracy: 1.0000\n",
      "Epoch 404/500\n",
      "8/8 [==============================] - 1s 94ms/step - loss: 4.2345e-04 - accuracy: 1.0000\n",
      "Epoch 405/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 4.3677e-04 - accuracy: 1.0000\n",
      "Epoch 406/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 4.6069e-04 - accuracy: 1.0000\n",
      "Epoch 407/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.0988e-04 - accuracy: 1.0000\n",
      "Epoch 408/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 4.1730e-04 - accuracy: 1.0000\n",
      "Epoch 409/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 4.4016e-04 - accuracy: 1.0000\n",
      "Epoch 410/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.1849e-04 - accuracy: 1.0000\n",
      "Epoch 411/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 3.9446e-04 - accuracy: 1.0000\n",
      "Epoch 412/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.6171e-04 - accuracy: 1.0000\n",
      "Epoch 413/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 4.2933e-04 - accuracy: 1.0000\n",
      "Epoch 414/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.1553e-04 - accuracy: 1.0000\n",
      "Epoch 415/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 4.9779e-04 - accuracy: 1.0000\n",
      "Epoch 416/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.9085e-04 - accuracy: 1.0000\n",
      "Epoch 417/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 3.9331e-04 - accuracy: 1.0000\n",
      "Epoch 418/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.9449e-04 - accuracy: 1.0000\n",
      "Epoch 419/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.8561e-04 - accuracy: 1.0000\n",
      "Epoch 420/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.6554e-04 - accuracy: 1.0000\n",
      "Epoch 421/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 3.9700e-04 - accuracy: 1.0000\n",
      "Epoch 422/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.9819e-04 - accuracy: 1.0000\n",
      "Epoch 423/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.8581e-04 - accuracy: 1.0000\n",
      "Epoch 424/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 4.0088e-04 - accuracy: 1.0000\n",
      "Epoch 425/500\n",
      "8/8 [==============================] - 1s 73ms/step - loss: 4.4684e-04 - accuracy: 1.0000\n",
      "Epoch 426/500\n",
      "8/8 [==============================] - 1s 82ms/step - loss: 3.7421e-04 - accuracy: 1.0000\n",
      "Epoch 427/500\n",
      "8/8 [==============================] - 1s 79ms/step - loss: 3.6368e-04 - accuracy: 1.0000\n",
      "Epoch 428/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 3.6386e-04 - accuracy: 1.0000\n",
      "Epoch 429/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 4.2550e-04 - accuracy: 1.0000\n",
      "Epoch 430/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 3.9639e-04 - accuracy: 1.0000\n",
      "Epoch 431/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 3.5775e-04 - accuracy: 1.0000\n",
      "Epoch 432/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.4807e-04 - accuracy: 1.0000\n",
      "Epoch 433/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.3619e-04 - accuracy: 1.0000\n",
      "Epoch 434/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.6245e-04 - accuracy: 1.0000\n",
      "Epoch 435/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.2575e-04 - accuracy: 1.0000\n",
      "Epoch 436/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.5986e-04 - accuracy: 1.0000\n",
      "Epoch 437/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.3834e-04 - accuracy: 1.0000\n",
      "Epoch 438/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.3708e-04 - accuracy: 1.0000\n",
      "Epoch 439/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.6154e-04 - accuracy: 1.0000\n",
      "Epoch 440/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.2825e-04 - accuracy: 1.0000\n",
      "Epoch 441/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.5987e-04 - accuracy: 1.0000\n",
      "Epoch 442/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.3991e-04 - accuracy: 1.0000\n",
      "Epoch 443/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.1075e-04 - accuracy: 1.0000\n",
      "Epoch 444/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.5560e-04 - accuracy: 1.0000\n",
      "Epoch 445/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.2390e-04 - accuracy: 1.0000\n",
      "Epoch 446/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.3313e-04 - accuracy: 1.0000\n",
      "Epoch 447/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.2972e-04 - accuracy: 1.0000\n",
      "Epoch 448/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 3.1647e-04 - accuracy: 1.0000\n",
      "Epoch 449/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.1513e-04 - accuracy: 1.0000\n",
      "Epoch 450/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.0205e-04 - accuracy: 1.0000\n",
      "Epoch 451/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.9751e-04 - accuracy: 1.0000\n",
      "Epoch 452/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 3.7445e-04 - accuracy: 1.0000\n",
      "Epoch 453/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 3.1438e-04 - accuracy: 1.0000\n",
      "Epoch 454/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.9258e-04 - accuracy: 1.0000\n",
      "Epoch 455/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 3.0947e-04 - accuracy: 1.0000\n",
      "Epoch 456/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.9251e-04 - accuracy: 1.0000\n",
      "Epoch 457/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.8427e-04 - accuracy: 1.0000\n",
      "Epoch 458/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.9612e-04 - accuracy: 1.0000\n",
      "Epoch 459/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.8751e-04 - accuracy: 1.0000\n",
      "Epoch 460/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.7930e-04 - accuracy: 1.0000\n",
      "Epoch 461/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.7696e-04 - accuracy: 1.0000\n",
      "Epoch 462/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 3.2051e-04 - accuracy: 1.0000\n",
      "Epoch 463/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.8174e-04 - accuracy: 1.0000\n",
      "Epoch 464/500\n",
      "8/8 [==============================] - 0s 61ms/step - loss: 2.7076e-04 - accuracy: 1.0000\n",
      "Epoch 465/500\n",
      "8/8 [==============================] - 0s 60ms/step - loss: 2.7115e-04 - accuracy: 1.0000\n",
      "Epoch 466/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.6669e-04 - accuracy: 1.0000\n",
      "Epoch 467/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.8306e-04 - accuracy: 1.0000\n",
      "Epoch 468/500\n",
      "8/8 [==============================] - 0s 59ms/step - loss: 2.7255e-04 - accuracy: 1.0000\n",
      "Epoch 469/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.6511e-04 - accuracy: 1.0000\n",
      "Epoch 470/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 3.0965e-04 - accuracy: 1.0000\n",
      "Epoch 471/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 2.7545e-04 - accuracy: 1.0000\n",
      "Epoch 472/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.7855e-04 - accuracy: 1.0000\n",
      "Epoch 473/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.8234e-04 - accuracy: 1.0000\n",
      "Epoch 474/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.6924e-04 - accuracy: 1.0000\n",
      "Epoch 475/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 3.0909e-04 - accuracy: 1.0000\n",
      "Epoch 476/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.9655e-04 - accuracy: 1.0000\n",
      "Epoch 477/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.5719e-04 - accuracy: 1.0000\n",
      "Epoch 478/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.4038e-04 - accuracy: 1.0000\n",
      "Epoch 479/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.5220e-04 - accuracy: 1.0000\n",
      "Epoch 480/500\n",
      "8/8 [==============================] - 1s 66ms/step - loss: 2.5551e-04 - accuracy: 1.0000\n",
      "Epoch 481/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.8852e-04 - accuracy: 1.0000\n",
      "Epoch 482/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.6928e-04 - accuracy: 1.0000\n",
      "Epoch 483/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.7081e-04 - accuracy: 1.0000\n",
      "Epoch 484/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.8938e-04 - accuracy: 1.0000\n",
      "Epoch 485/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.3952e-04 - accuracy: 1.0000\n",
      "Epoch 486/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.6031e-04 - accuracy: 1.0000\n",
      "Epoch 487/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.6143e-04 - accuracy: 1.0000\n",
      "Epoch 488/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3938e-04 - accuracy: 1.0000\n",
      "Epoch 489/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3137e-04 - accuracy: 1.0000\n",
      "Epoch 490/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.3235e-04 - accuracy: 1.0000\n",
      "Epoch 491/500\n",
      "8/8 [==============================] - 0s 62ms/step - loss: 2.3976e-04 - accuracy: 1.0000\n",
      "Epoch 492/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.3188e-04 - accuracy: 1.0000\n",
      "Epoch 493/500\n",
      "8/8 [==============================] - 1s 68ms/step - loss: 2.3860e-04 - accuracy: 1.0000\n",
      "Epoch 494/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 2.4660e-04 - accuracy: 1.0000\n",
      "Epoch 495/500\n",
      "8/8 [==============================] - 1s 65ms/step - loss: 2.1932e-04 - accuracy: 1.0000\n",
      "Epoch 496/500\n",
      "8/8 [==============================] - 1s 62ms/step - loss: 2.1574e-04 - accuracy: 1.0000\n",
      "Epoch 497/500\n",
      "8/8 [==============================] - 1s 64ms/step - loss: 2.5328e-04 - accuracy: 1.0000\n",
      "Epoch 498/500\n",
      "8/8 [==============================] - 1s 63ms/step - loss: 2.0776e-04 - accuracy: 1.0000\n",
      "Epoch 499/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.2609e-04 - accuracy: 1.0000\n",
      "Epoch 500/500\n",
      "8/8 [==============================] - 0s 58ms/step - loss: 2.2583e-04 - accuracy: 1.0000\n",
      "2/2 [==============================] - 14s 18ms/step - loss: 1.5318e-04 - accuracy: 1.0000\n",
      "Test Loss: 0.00015318494115490466, Test Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 20\n",
    "output_dim = 3\n",
    "num_layers = 3\n",
    "num_knots = 5\n",
    "num_epochs = 500\n",
    "learning_rate = 0.05\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(16).shuffle(buffer_size=100)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(16)\n",
    "\n",
    "model = TensorFlowKAN(input_dim, hidden_dim, output_dim, num_layers, num_knots)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_dataset, epochs=num_epochs)\n",
    "\n",
    "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
    "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5050dd9-e02b-49d3-87f9-f4cba299c89c",
   "metadata": {},
   "source": [
    "# Multi Layer Perceptrons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "77cc4041-8967-4583-8a12-a662a2b86342",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "efb3ecac-a552-4c11-94a7-2f290091eea6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Loss: 0.9599684700369835\n",
      "Epoch [2/500], Loss: 0.7031642124056816\n",
      "Epoch [3/500], Loss: 0.5391526818275452\n",
      "Epoch [4/500], Loss: 0.4470079317688942\n",
      "Epoch [5/500], Loss: 0.39584067836403847\n",
      "Epoch [6/500], Loss: 0.3200357463210821\n",
      "Epoch [7/500], Loss: 0.2531310822814703\n",
      "Epoch [8/500], Loss: 0.18112275283783674\n",
      "Epoch [9/500], Loss: 0.13655362324789166\n",
      "Epoch [10/500], Loss: 0.1065359462518245\n",
      "Epoch [11/500], Loss: 0.08960304176434875\n",
      "Epoch [12/500], Loss: 0.0821549731772393\n",
      "Epoch [13/500], Loss: 0.06753220810787752\n",
      "Epoch [14/500], Loss: 0.09448243724182248\n",
      "Epoch [15/500], Loss: 0.07362987729720771\n",
      "Epoch [16/500], Loss: 0.0812568892724812\n",
      "Epoch [17/500], Loss: 0.060002128477208316\n",
      "Epoch [18/500], Loss: 0.05889482802012935\n",
      "Epoch [19/500], Loss: 0.054784952313639224\n",
      "Epoch [20/500], Loss: 0.05500718264374882\n",
      "Epoch [21/500], Loss: 0.0723117520683445\n",
      "Epoch [22/500], Loss: 0.051754513406194746\n",
      "Epoch [23/500], Loss: 0.050828117033233866\n",
      "Epoch [24/500], Loss: 0.06649793830001727\n",
      "Epoch [25/500], Loss: 0.0648185305763036\n",
      "Epoch [26/500], Loss: 0.07578567310702056\n",
      "Epoch [27/500], Loss: 0.04870167176704854\n",
      "Epoch [28/500], Loss: 0.08435781074513216\n",
      "Epoch [29/500], Loss: 0.04758939173188992\n",
      "Epoch [30/500], Loss: 0.05245203536469489\n",
      "Epoch [31/500], Loss: 0.04779709642753005\n",
      "Epoch [32/500], Loss: 0.049756341584725305\n",
      "Epoch [33/500], Loss: 0.049331551388604566\n",
      "Epoch [34/500], Loss: 0.04303770081605762\n",
      "Epoch [35/500], Loss: 0.0595801777089946\n",
      "Epoch [36/500], Loss: 0.04242424252151977\n",
      "Epoch [37/500], Loss: 0.06504861540452112\n",
      "Epoch [38/500], Loss: 0.06279272906249389\n",
      "Epoch [39/500], Loss: 0.04436113276460674\n",
      "Epoch [40/500], Loss: 0.0520329573773779\n",
      "Epoch [41/500], Loss: 0.05885564078198513\n",
      "Epoch [42/500], Loss: 0.05202983890194446\n",
      "Epoch [43/500], Loss: 0.04585205602052156\n",
      "Epoch [44/500], Loss: 0.04503647543606348\n",
      "Epoch [45/500], Loss: 0.04150263743940741\n",
      "Epoch [46/500], Loss: 0.052407128998311237\n",
      "Epoch [47/500], Loss: 0.059545203897869214\n",
      "Epoch [48/500], Loss: 0.0624937878747005\n",
      "Epoch [49/500], Loss: 0.043208956834860146\n",
      "Epoch [50/500], Loss: 0.04794890873017721\n",
      "Epoch [51/500], Loss: 0.040967380948131904\n",
      "Epoch [52/500], Loss: 0.04489936676691286\n",
      "Epoch [53/500], Loss: 0.037625491810103995\n",
      "Epoch [54/500], Loss: 0.0480756601318717\n",
      "Epoch [55/500], Loss: 0.044294805789832026\n",
      "Epoch [56/500], Loss: 0.039832225898862816\n",
      "Epoch [57/500], Loss: 0.054813043687317986\n",
      "Epoch [58/500], Loss: 0.05284567395574413\n",
      "Epoch [59/500], Loss: 0.046011700498638675\n",
      "Epoch [60/500], Loss: 0.06019867173017701\n",
      "Epoch [61/500], Loss: 0.06538768176687881\n",
      "Epoch [62/500], Loss: 0.04690816011861898\n",
      "Epoch [63/500], Loss: 0.03889727991190739\n",
      "Epoch [64/500], Loss: 0.041496240788546856\n",
      "Epoch [65/500], Loss: 0.04024530258902814\n",
      "Epoch [66/500], Loss: 0.042871010256931186\n",
      "Epoch [67/500], Loss: 0.04167267298907973\n",
      "Epoch [68/500], Loss: 0.044035323608113686\n",
      "Epoch [69/500], Loss: 0.03984026994294254\n",
      "Epoch [70/500], Loss: 0.03933514014352113\n",
      "Epoch [71/500], Loss: 0.03558470605639741\n",
      "Epoch [72/500], Loss: 0.044988300040131435\n",
      "Epoch [73/500], Loss: 0.04249451605573995\n",
      "Epoch [74/500], Loss: 0.045169598422944546\n",
      "Epoch [75/500], Loss: 0.04025451690540649\n",
      "Epoch [76/500], Loss: 0.040884879388613626\n",
      "Epoch [77/500], Loss: 0.05248457100969972\n",
      "Epoch [78/500], Loss: 0.03821845444326755\n",
      "Epoch [79/500], Loss: 0.03849460916535463\n",
      "Epoch [80/500], Loss: 0.0366505968850106\n",
      "Epoch [81/500], Loss: 0.03489582038309891\n",
      "Epoch [82/500], Loss: 0.053283736619050615\n",
      "Epoch [83/500], Loss: 0.03462066111387685\n",
      "Epoch [84/500], Loss: 0.035532035362848546\n",
      "Epoch [85/500], Loss: 0.05076327332062647\n",
      "Epoch [86/500], Loss: 0.049598038487602025\n",
      "Epoch [87/500], Loss: 0.0383456415765977\n",
      "Epoch [88/500], Loss: 0.03667141855112277\n",
      "Epoch [89/500], Loss: 0.03534290901006898\n",
      "Epoch [90/500], Loss: 0.03403154588886537\n",
      "Epoch [91/500], Loss: 0.03376541046600323\n",
      "Epoch [92/500], Loss: 0.037316943242331035\n",
      "Epoch [93/500], Loss: 0.03263979183975607\n",
      "Epoch [94/500], Loss: 0.030203451693523675\n",
      "Epoch [95/500], Loss: 0.029035918505542213\n",
      "Epoch [96/500], Loss: 0.029537499642174225\n",
      "Epoch [97/500], Loss: 0.02982271439395845\n",
      "Epoch [98/500], Loss: 0.027885865660209674\n",
      "Epoch [99/500], Loss: 0.030777533378568478\n",
      "Epoch [100/500], Loss: 0.026718773657194106\n",
      "Epoch [101/500], Loss: 0.03894690453307703\n",
      "Epoch [102/500], Loss: 0.026688629601267166\n",
      "Epoch [103/500], Loss: 0.025589965131075587\n",
      "Epoch [104/500], Loss: 0.02563552313768014\n",
      "Epoch [105/500], Loss: 0.02732295002988394\n",
      "Epoch [106/500], Loss: 0.025688013345643412\n",
      "Epoch [107/500], Loss: 0.025112222145253327\n",
      "Epoch [108/500], Loss: 0.025910276162903756\n",
      "Epoch [109/500], Loss: 0.04326911331008887\n",
      "Epoch [110/500], Loss: 0.026472400117199868\n",
      "Epoch [111/500], Loss: 0.027058074134401977\n",
      "Epoch [112/500], Loss: 0.02591431275868672\n",
      "Epoch [113/500], Loss: 0.023576249419420492\n",
      "Epoch [114/500], Loss: 0.02277420375321526\n",
      "Epoch [115/500], Loss: 0.020528087065031286\n",
      "Epoch [116/500], Loss: 0.01954037948689802\n",
      "Epoch [117/500], Loss: 0.021044615974460612\n",
      "Epoch [118/500], Loss: 0.02489604959191638\n",
      "Epoch [119/500], Loss: 0.017280879688769346\n",
      "Epoch [120/500], Loss: 0.020873986515653087\n",
      "Epoch [121/500], Loss: 0.02698073033388937\n",
      "Epoch [122/500], Loss: 0.014402084547327831\n",
      "Epoch [123/500], Loss: 0.020491430370384478\n",
      "Epoch [124/500], Loss: 0.018782860723149497\n",
      "Epoch [125/500], Loss: 0.016809203571028775\n",
      "Epoch [126/500], Loss: 0.016788376875410904\n",
      "Epoch [127/500], Loss: 0.016612533778243233\n",
      "Epoch [128/500], Loss: 0.015094996389962034\n",
      "Epoch [129/500], Loss: 0.022367135807144223\n",
      "Epoch [130/500], Loss: 0.020981351844056917\n",
      "Epoch [131/500], Loss: 0.03930509716064989\n",
      "Epoch [132/500], Loss: 0.025408317473193165\n",
      "Epoch [133/500], Loss: 0.03413982090569334\n",
      "Epoch [134/500], Loss: 0.029190688457674696\n",
      "Epoch [135/500], Loss: 0.022323552231682697\n",
      "Epoch [136/500], Loss: 0.013572304877015995\n",
      "Epoch [137/500], Loss: 0.017953255063730467\n",
      "Epoch [138/500], Loss: 0.014207289532350842\n",
      "Epoch [139/500], Loss: 0.011727698580216384\n",
      "Epoch [140/500], Loss: 0.015867052285102545\n",
      "Epoch [141/500], Loss: 0.014913952227288974\n",
      "Epoch [142/500], Loss: 0.015308035657653818\n",
      "Epoch [143/500], Loss: 0.02885712089118897\n",
      "Epoch [144/500], Loss: 0.016894913620490115\n",
      "Epoch [145/500], Loss: 0.011711326628528695\n",
      "Epoch [146/500], Loss: 0.011470755485788686\n",
      "Epoch [147/500], Loss: 0.010767835613023635\n",
      "Epoch [148/500], Loss: 0.012160670561570441\n",
      "Epoch [149/500], Loss: 0.011089156022080715\n",
      "Epoch [150/500], Loss: 0.013721430118948774\n",
      "Epoch [151/500], Loss: 0.013240608082242034\n",
      "Epoch [152/500], Loss: 0.010802377321851964\n",
      "Epoch [153/500], Loss: 0.014864208995277295\n",
      "Epoch [154/500], Loss: 0.01251259123091586\n",
      "Epoch [155/500], Loss: 0.007673456123939104\n",
      "Epoch [156/500], Loss: 0.011897951426362852\n",
      "Epoch [157/500], Loss: 0.01047180555929117\n",
      "Epoch [158/500], Loss: 0.0070414773381344276\n",
      "Epoch [159/500], Loss: 0.010150644069653936\n",
      "Epoch [160/500], Loss: 0.006646628028306623\n",
      "Epoch [161/500], Loss: 0.008017741186449712\n",
      "Epoch [162/500], Loss: 0.009002367560242419\n",
      "Epoch [163/500], Loss: 0.008217341293857316\n",
      "Epoch [164/500], Loss: 0.005822641218401259\n",
      "Epoch [165/500], Loss: 0.010368443534389371\n",
      "Epoch [166/500], Loss: 0.006649795432622341\n",
      "Epoch [167/500], Loss: 0.00672217385363183\n",
      "Epoch [168/500], Loss: 0.006551037723227182\n",
      "Epoch [169/500], Loss: 0.007646807061973959\n",
      "Epoch [170/500], Loss: 0.006455081207604962\n",
      "Epoch [171/500], Loss: 0.006586383871763246\n",
      "Epoch [172/500], Loss: 0.005385691903939005\n",
      "Epoch [173/500], Loss: 0.007740413912642907\n",
      "Epoch [174/500], Loss: 0.004673173312767176\n",
      "Epoch [175/500], Loss: 0.008044133198382042\n",
      "Epoch [176/500], Loss: 0.005444715882731543\n",
      "Epoch [177/500], Loss: 0.006007358764236415\n",
      "Epoch [178/500], Loss: 0.005575942396717437\n",
      "Epoch [179/500], Loss: 0.00464727945291088\n",
      "Epoch [180/500], Loss: 0.004764680099469842\n",
      "Epoch [181/500], Loss: 0.004619507693860214\n",
      "Epoch [182/500], Loss: 0.005193452713228908\n",
      "Epoch [183/500], Loss: 0.004173784933982461\n",
      "Epoch [184/500], Loss: 0.004705147103322815\n",
      "Epoch [185/500], Loss: 0.005180386133361026\n",
      "Epoch [186/500], Loss: 0.005073080035799649\n",
      "Epoch [187/500], Loss: 0.003836567723396911\n",
      "Epoch [188/500], Loss: 0.004844177092763857\n",
      "Epoch [189/500], Loss: 0.003954604631871916\n",
      "Epoch [190/500], Loss: 0.0038400532384912367\n",
      "Epoch [191/500], Loss: 0.0034046557166220737\n",
      "Epoch [192/500], Loss: 0.003812286777247209\n",
      "Epoch [193/500], Loss: 0.0037929734298813855\n",
      "Epoch [194/500], Loss: 0.004372500910903909\n",
      "Epoch [195/500], Loss: 0.004612528260622639\n",
      "Epoch [196/500], Loss: 0.008221447700634599\n",
      "Epoch [197/500], Loss: 0.019522573626318263\n",
      "Epoch [198/500], Loss: 0.06365791660863351\n",
      "Epoch [199/500], Loss: 0.04174129132297821\n",
      "Epoch [200/500], Loss: 0.02281969171053788\n",
      "Epoch [201/500], Loss: 0.037850881761642086\n",
      "Epoch [202/500], Loss: 0.013145898878065054\n",
      "Epoch [203/500], Loss: 0.005469864717269957\n",
      "Epoch [204/500], Loss: 0.006806082197954311\n",
      "Epoch [205/500], Loss: 0.005525715095700434\n",
      "Epoch [206/500], Loss: 0.007294206909136847\n",
      "Epoch [207/500], Loss: 0.005875795740223566\n",
      "Epoch [208/500], Loss: 0.0034776468050949916\n",
      "Epoch [209/500], Loss: 0.003958456632972229\n",
      "Epoch [210/500], Loss: 0.003124223503164103\n",
      "Epoch [211/500], Loss: 0.00291426561489061\n",
      "Epoch [212/500], Loss: 0.0030196645857358817\n",
      "Epoch [213/500], Loss: 0.002634495395795966\n",
      "Epoch [214/500], Loss: 0.0028762523827481346\n",
      "Epoch [215/500], Loss: 0.0027206116792513058\n",
      "Epoch [216/500], Loss: 0.0033643118781583325\n",
      "Epoch [217/500], Loss: 0.0028172904221719364\n",
      "Epoch [218/500], Loss: 0.0023399078404509055\n",
      "Epoch [219/500], Loss: 0.0025801579886319814\n",
      "Epoch [220/500], Loss: 0.0024071377811196726\n",
      "Epoch [221/500], Loss: 0.002286241832621272\n",
      "Epoch [222/500], Loss: 0.002381053045610315\n",
      "Epoch [223/500], Loss: 0.002706649602714606\n",
      "Epoch [224/500], Loss: 0.0021503343551785292\n",
      "Epoch [225/500], Loss: 0.002417117228560528\n",
      "Epoch [226/500], Loss: 0.002343260201087105\n",
      "Epoch [227/500], Loss: 0.0026366426516233332\n",
      "Epoch [228/500], Loss: 0.002591246906831657\n",
      "Epoch [229/500], Loss: 0.0032677363445827723\n",
      "Epoch [230/500], Loss: 0.002201114434058127\n",
      "Epoch [231/500], Loss: 0.003813742791862751\n",
      "Epoch [232/500], Loss: 0.0019448820271463774\n",
      "Epoch [233/500], Loss: 0.0029284514057508204\n",
      "Epoch [234/500], Loss: 0.0021961169032920225\n",
      "Epoch [235/500], Loss: 0.002802092514684773\n",
      "Epoch [236/500], Loss: 0.0024523093737514046\n",
      "Epoch [237/500], Loss: 0.0018797668456045358\n",
      "Epoch [238/500], Loss: 0.0024516437042620964\n",
      "Epoch [239/500], Loss: 0.0016273278297376237\n",
      "Epoch [240/500], Loss: 0.002075726666589617\n",
      "Epoch [241/500], Loss: 0.0016755561496211158\n",
      "Epoch [242/500], Loss: 0.0016877593167237137\n",
      "Epoch [243/500], Loss: 0.0016827725988832754\n",
      "Epoch [244/500], Loss: 0.001959455373878427\n",
      "Epoch [245/500], Loss: 0.0017546315084473463\n",
      "Epoch [246/500], Loss: 0.0023315159673984454\n",
      "Epoch [247/500], Loss: 0.0016812757698971836\n",
      "Epoch [248/500], Loss: 0.0026652129881767905\n",
      "Epoch [249/500], Loss: 0.0014362352933687816\n",
      "Epoch [250/500], Loss: 0.0017942333108749153\n",
      "Epoch [251/500], Loss: 0.0015363230977527564\n",
      "Epoch [252/500], Loss: 0.0014449190011873725\n",
      "Epoch [253/500], Loss: 0.0018011729462159565\n",
      "Epoch [254/500], Loss: 0.0016738199977339718\n",
      "Epoch [255/500], Loss: 0.0013237581340490578\n",
      "Epoch [256/500], Loss: 0.0018681227846286674\n",
      "Epoch [257/500], Loss: 0.0013722112342975379\n",
      "Epoch [258/500], Loss: 0.001318350331075635\n",
      "Epoch [259/500], Loss: 0.001308481247178861\n",
      "Epoch [260/500], Loss: 0.0014931097265389326\n",
      "Epoch [261/500], Loss: 0.0013233151751705918\n",
      "Epoch [262/500], Loss: 0.0012525370180469508\n",
      "Epoch [263/500], Loss: 0.0013002781093405247\n",
      "Epoch [264/500], Loss: 0.0012262951456136761\n",
      "Epoch [265/500], Loss: 0.0011795139878358896\n",
      "Epoch [266/500], Loss: 0.0012337131048525407\n",
      "Epoch [267/500], Loss: 0.0020768974686689035\n",
      "Epoch [268/500], Loss: 0.001196349266422203\n",
      "Epoch [269/500], Loss: 0.0013700454142053786\n",
      "Epoch [270/500], Loss: 0.0010860820435709684\n",
      "Epoch [271/500], Loss: 0.0012500480856942886\n",
      "Epoch [272/500], Loss: 0.0017752252260834211\n",
      "Epoch [273/500], Loss: 0.0009742228521645302\n",
      "Epoch [274/500], Loss: 0.0017194582343336151\n",
      "Epoch [275/500], Loss: 0.001317747822668025\n",
      "Epoch [276/500], Loss: 0.001282887510569708\n",
      "Epoch [277/500], Loss: 0.0010439025409141323\n",
      "Epoch [278/500], Loss: 0.0011510894503317104\n",
      "Epoch [279/500], Loss: 0.0012552566408885468\n",
      "Epoch [280/500], Loss: 0.001186372860956908\n",
      "Epoch [281/500], Loss: 0.0011671858360386977\n",
      "Epoch [282/500], Loss: 0.0011192682667342524\n",
      "Epoch [283/500], Loss: 0.0010179698644634527\n",
      "Epoch [284/500], Loss: 0.0009649948124206276\n",
      "Epoch [285/500], Loss: 0.0010701801957111456\n",
      "Epoch [286/500], Loss: 0.001252314680186828\n",
      "Epoch [287/500], Loss: 0.0010365940527208295\n",
      "Epoch [288/500], Loss: 0.001032910391444375\n",
      "Epoch [289/500], Loss: 0.0009061361782869426\n",
      "Epoch [290/500], Loss: 0.000956132809918131\n",
      "Epoch [291/500], Loss: 0.00105138694287632\n",
      "Epoch [292/500], Loss: 0.0009462602328902392\n",
      "Epoch [293/500], Loss: 0.0009213743426244037\n",
      "Epoch [294/500], Loss: 0.0009539768352055944\n",
      "Epoch [295/500], Loss: 0.0008473014354706265\n",
      "Epoch [296/500], Loss: 0.0008731343361887411\n",
      "Epoch [297/500], Loss: 0.0011946427584916819\n",
      "Epoch [298/500], Loss: 0.0010963319396068982\n",
      "Epoch [299/500], Loss: 0.0009508127504602726\n",
      "Epoch [300/500], Loss: 0.0012019571462360545\n",
      "Epoch [301/500], Loss: 0.0015219288507068995\n",
      "Epoch [302/500], Loss: 0.0010386280403054116\n",
      "Epoch [303/500], Loss: 0.0012614841498361784\n",
      "Epoch [304/500], Loss: 0.0008742210475247703\n",
      "Epoch [305/500], Loss: 0.0008575467173841389\n",
      "Epoch [306/500], Loss: 0.0007918155292827578\n",
      "Epoch [307/500], Loss: 0.0007627437330484099\n",
      "Epoch [308/500], Loss: 0.0007623787883019872\n",
      "Epoch [309/500], Loss: 0.0009333062932910252\n",
      "Epoch [310/500], Loss: 0.0007722256260649374\n",
      "Epoch [311/500], Loss: 0.0007349960930582711\n",
      "Epoch [312/500], Loss: 0.0007303795090862764\n",
      "Epoch [313/500], Loss: 0.0007195830230557476\n",
      "Epoch [314/500], Loss: 0.0007280090282506535\n",
      "Epoch [315/500], Loss: 0.0007027389043550158\n",
      "Epoch [316/500], Loss: 0.0007078675368887843\n",
      "Epoch [317/500], Loss: 0.000753611022446421\n",
      "Epoch [318/500], Loss: 0.0007960579596328898\n",
      "Epoch [319/500], Loss: 0.0007722577218487459\n",
      "Epoch [320/500], Loss: 0.0006774011561887505\n",
      "Epoch [321/500], Loss: 0.0006828084591461447\n",
      "Epoch [322/500], Loss: 0.0006651479072843358\n",
      "Epoch [323/500], Loss: 0.0006379392464168632\n",
      "Epoch [324/500], Loss: 0.0006983819334038799\n",
      "Epoch [325/500], Loss: 0.0007000896666546907\n",
      "Epoch [326/500], Loss: 0.0006784518324138844\n",
      "Epoch [327/500], Loss: 0.0006951600372531175\n",
      "Epoch [328/500], Loss: 0.0007554008492611786\n",
      "Epoch [329/500], Loss: 0.000738669985153706\n",
      "Epoch [330/500], Loss: 0.0007034703817510035\n",
      "Epoch [331/500], Loss: 0.0006668000435183785\n",
      "Epoch [332/500], Loss: 0.000585317198328994\n",
      "Epoch [333/500], Loss: 0.0006828354258487934\n",
      "Epoch [334/500], Loss: 0.0006004484795880671\n",
      "Epoch [335/500], Loss: 0.0008844028824341876\n",
      "Epoch [336/500], Loss: 0.0005642674263484082\n",
      "Epoch [337/500], Loss: 0.0005953595291998681\n",
      "Epoch [338/500], Loss: 0.0009915982830079884\n",
      "Epoch [339/500], Loss: 0.0006425037063308991\n",
      "Epoch [340/500], Loss: 0.0008544446272935602\n",
      "Epoch [341/500], Loss: 0.0005537159618711485\n",
      "Epoch [342/500], Loss: 0.0006948494494736224\n",
      "Epoch [343/500], Loss: 0.0005725093851651764\n",
      "Epoch [344/500], Loss: 0.0005927863995793814\n",
      "Epoch [345/500], Loss: 0.0005682584352513231\n",
      "Epoch [346/500], Loss: 0.0005378396820105991\n",
      "Epoch [347/500], Loss: 0.0005990116812881752\n",
      "Epoch [348/500], Loss: 0.0005294583423847143\n",
      "Epoch [349/500], Loss: 0.0005871335952178924\n",
      "Epoch [350/500], Loss: 0.0005145183636017236\n",
      "Epoch [351/500], Loss: 0.0005107043598382432\n",
      "Epoch [352/500], Loss: 0.0005428233980495634\n",
      "Epoch [353/500], Loss: 0.0005100566390723316\n",
      "Epoch [354/500], Loss: 0.000512221945626834\n",
      "Epoch [355/500], Loss: 0.00047578314081420103\n",
      "Epoch [356/500], Loss: 0.0005185906287579201\n",
      "Epoch [357/500], Loss: 0.00046932904666618924\n",
      "Epoch [358/500], Loss: 0.0005703763329165668\n",
      "Epoch [359/500], Loss: 0.00047924989598158163\n",
      "Epoch [360/500], Loss: 0.0005199751919064965\n",
      "Epoch [361/500], Loss: 0.00045530316629083245\n",
      "Epoch [362/500], Loss: 0.000769981093185379\n",
      "Epoch [363/500], Loss: 0.0007033605741639803\n",
      "Epoch [364/500], Loss: 0.0005780235894654595\n",
      "Epoch [365/500], Loss: 0.000455306921310239\n",
      "Epoch [366/500], Loss: 0.0004568622567830971\n",
      "Epoch [367/500], Loss: 0.00045491288506127603\n",
      "Epoch [368/500], Loss: 0.00048329471382135836\n",
      "Epoch [369/500], Loss: 0.0004436993414742574\n",
      "Epoch [370/500], Loss: 0.00043494164448532047\n",
      "Epoch [371/500], Loss: 0.0004682314842057167\n",
      "Epoch [372/500], Loss: 0.0004347402754660834\n",
      "Epoch [373/500], Loss: 0.000456451722982365\n",
      "Epoch [374/500], Loss: 0.00042074606489705957\n",
      "Epoch [375/500], Loss: 0.00042088548741503473\n",
      "Epoch [376/500], Loss: 0.00040545217484577734\n",
      "Epoch [377/500], Loss: 0.00041861117213670695\n",
      "Epoch [378/500], Loss: 0.0003983248152081842\n",
      "Epoch [379/500], Loss: 0.0004250257623681364\n",
      "Epoch [380/500], Loss: 0.0003911752476142283\n",
      "Epoch [381/500], Loss: 0.0004230915657217338\n",
      "Epoch [382/500], Loss: 0.0003971472582406932\n",
      "Epoch [383/500], Loss: 0.0004009544214085281\n",
      "Epoch [384/500], Loss: 0.0003848334581562085\n",
      "Epoch [385/500], Loss: 0.00041469778614100505\n",
      "Epoch [386/500], Loss: 0.0003873598266039835\n",
      "Epoch [387/500], Loss: 0.0003920942847024378\n",
      "Epoch [388/500], Loss: 0.00036598257055686645\n",
      "Epoch [389/500], Loss: 0.00036080469408261706\n",
      "Epoch [390/500], Loss: 0.00040466245364711995\n",
      "Epoch [391/500], Loss: 0.0004444413050350704\n",
      "Epoch [392/500], Loss: 0.00037483813321159687\n",
      "Epoch [393/500], Loss: 0.00036906048555351845\n",
      "Epoch [394/500], Loss: 0.00038382602190001336\n",
      "Epoch [395/500], Loss: 0.00036774529865901684\n",
      "Epoch [396/500], Loss: 0.0003562530484941817\n",
      "Epoch [397/500], Loss: 0.00035611514678635103\n",
      "Epoch [398/500], Loss: 0.000403647295613041\n",
      "Epoch [399/500], Loss: 0.00033155598748635384\n",
      "Epoch [400/500], Loss: 0.00036292681323857323\n",
      "Epoch [401/500], Loss: 0.00033385354144854773\n",
      "Epoch [402/500], Loss: 0.00034825193831977685\n",
      "Epoch [403/500], Loss: 0.0003481822502635623\n",
      "Epoch [404/500], Loss: 0.0003395182907297567\n",
      "Epoch [405/500], Loss: 0.0003615197917454793\n",
      "Epoch [406/500], Loss: 0.0003333267509333382\n",
      "Epoch [407/500], Loss: 0.0003347199135816936\n",
      "Epoch [408/500], Loss: 0.000319318184857309\n",
      "Epoch [409/500], Loss: 0.00031601881982989966\n",
      "Epoch [410/500], Loss: 0.0003165309000792149\n",
      "Epoch [411/500], Loss: 0.00034517579300086254\n",
      "Epoch [412/500], Loss: 0.0003278997736515521\n",
      "Epoch [413/500], Loss: 0.00037701701620562744\n",
      "Epoch [414/500], Loss: 0.0003167248594309058\n",
      "Epoch [415/500], Loss: 0.0003011859800494676\n",
      "Epoch [416/500], Loss: 0.0003061012404828034\n",
      "Epoch [417/500], Loss: 0.0003115721503519353\n",
      "Epoch [418/500], Loss: 0.00033607578689043294\n",
      "Epoch [419/500], Loss: 0.00031289329797346\n",
      "Epoch [420/500], Loss: 0.0002863037326790163\n",
      "Epoch [421/500], Loss: 0.00029037004696874646\n",
      "Epoch [422/500], Loss: 0.0003200907615905635\n",
      "Epoch [423/500], Loss: 0.0003245892003249651\n",
      "Epoch [424/500], Loss: 0.0003229798736583689\n",
      "Epoch [425/500], Loss: 0.0002857711491515147\n",
      "Epoch [426/500], Loss: 0.0003298714900097366\n",
      "Epoch [427/500], Loss: 0.00031199234058476577\n",
      "Epoch [428/500], Loss: 0.00027695709050590267\n",
      "Epoch [429/500], Loss: 0.00026946747304634755\n",
      "Epoch [430/500], Loss: 0.00037208760943485686\n",
      "Epoch [431/500], Loss: 0.0002746956926280575\n",
      "Epoch [432/500], Loss: 0.00027518965938355677\n",
      "Epoch [433/500], Loss: 0.00030116597332607853\n",
      "Epoch [434/500], Loss: 0.00028913862138324475\n",
      "Epoch [435/500], Loss: 0.0003553161499780799\n",
      "Epoch [436/500], Loss: 0.000274032550805714\n",
      "Epoch [437/500], Loss: 0.0002636764961039262\n",
      "Epoch [438/500], Loss: 0.0002650591899282517\n",
      "Epoch [439/500], Loss: 0.00026505060532144853\n",
      "Epoch [440/500], Loss: 0.0002944025760314162\n",
      "Epoch [441/500], Loss: 0.00023765407913955983\n",
      "Epoch [442/500], Loss: 0.00040113377394845884\n",
      "Epoch [443/500], Loss: 0.00022128744518923327\n",
      "Epoch [444/500], Loss: 0.00034714742980668234\n",
      "Epoch [445/500], Loss: 0.0002448813892197421\n",
      "Epoch [446/500], Loss: 0.00028920335444126977\n",
      "Epoch [447/500], Loss: 0.00025866142372876766\n",
      "Epoch [448/500], Loss: 0.00027902052818262746\n",
      "Epoch [449/500], Loss: 0.00039830493744830164\n",
      "Epoch [450/500], Loss: 0.000270557429640661\n",
      "Epoch [451/500], Loss: 0.0003264112659735474\n",
      "Epoch [452/500], Loss: 0.00024103045586798544\n",
      "Epoch [453/500], Loss: 0.0002582915902848981\n",
      "Epoch [454/500], Loss: 0.0002280086945347648\n",
      "Epoch [455/500], Loss: 0.0002183352873963429\n",
      "Epoch [456/500], Loss: 0.0002505097718117355\n",
      "Epoch [457/500], Loss: 0.00024470642604512705\n",
      "Epoch [458/500], Loss: 0.00036535381740065986\n",
      "Epoch [459/500], Loss: 0.00021702732534478741\n",
      "Epoch [460/500], Loss: 0.00032339490496724466\n",
      "Epoch [461/500], Loss: 0.00024997759028622113\n",
      "Epoch [462/500], Loss: 0.00026118014571618886\n",
      "Epoch [463/500], Loss: 0.0003325091228703059\n",
      "Epoch [464/500], Loss: 0.00022328586965159047\n",
      "Epoch [465/500], Loss: 0.00025492658743075935\n",
      "Epoch [466/500], Loss: 0.00023131199959181004\n",
      "Epoch [467/500], Loss: 0.00023688344711558784\n",
      "Epoch [468/500], Loss: 0.00020588296406032214\n",
      "Epoch [469/500], Loss: 0.00020089425180447318\n",
      "Epoch [470/500], Loss: 0.00021166542702388824\n",
      "Epoch [471/500], Loss: 0.0002164197559961778\n",
      "Epoch [472/500], Loss: 0.00020766143907735568\n",
      "Epoch [473/500], Loss: 0.0002974492482614721\n",
      "Epoch [474/500], Loss: 0.0002448331000550752\n",
      "Epoch [475/500], Loss: 0.00020684306423390808\n",
      "Epoch [476/500], Loss: 0.0002060869002207255\n",
      "Epoch [477/500], Loss: 0.00019817441761915688\n",
      "Epoch [478/500], Loss: 0.00020496760346588871\n",
      "Epoch [479/500], Loss: 0.0002650205831287167\n",
      "Epoch [480/500], Loss: 0.00019104874226627544\n",
      "Epoch [481/500], Loss: 0.00025405757176599764\n",
      "Epoch [482/500], Loss: 0.00023991884832241794\n",
      "Epoch [483/500], Loss: 0.0001912560632870708\n",
      "Epoch [484/500], Loss: 0.00018807518609520457\n",
      "Epoch [485/500], Loss: 0.00022741135421711078\n",
      "Epoch [486/500], Loss: 0.00018025252467168684\n",
      "Epoch [487/500], Loss: 0.00020156560478312713\n",
      "Epoch [488/500], Loss: 0.00018516990471795225\n",
      "Epoch [489/500], Loss: 0.00017632995331240409\n",
      "Epoch [490/500], Loss: 0.0001757888325641943\n",
      "Epoch [491/500], Loss: 0.00018000866037937158\n",
      "Epoch [492/500], Loss: 0.0001731912905995614\n",
      "Epoch [493/500], Loss: 0.00016917910397706848\n",
      "Epoch [494/500], Loss: 0.00023645152114681878\n",
      "Epoch [495/500], Loss: 0.00017475207675943238\n",
      "Epoch [496/500], Loss: 0.00019321496989732623\n",
      "Epoch [497/500], Loss: 0.00017443945151285334\n",
      "Epoch [498/500], Loss: 0.00020243329995395243\n",
      "Epoch [499/500], Loss: 0.0001637774883676002\n",
      "Epoch [500/500], Loss: 0.00024936652610563215\n",
      "Test Loss: 0.009217552142217755, Accuracy: 100.0%\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_dim = 4\n",
    "hidden_dim = 10\n",
    "output_dim = 3\n",
    "num_epochs = 500\n",
    "learning_rate = 0.01\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "\n",
    "mlp_model = MLP(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate Functions\n",
    "def mlp_train_model(model, train_loader, criterion, optimizer, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "def mlp_evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Loss: {total_loss/len(test_loader)}, Accuracy: {accuracy}%\")\n",
    "\n",
    "mlp_train_model(mlp_model, train_loader, criterion, optimizer, num_epochs)\n",
    "mlp_evaluate_model(mlp_model, test_loader, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527eceb6-3823-43b1-9571-e0821848b356",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877af43-f53b-41fd-818c-e45c1b44b9b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
